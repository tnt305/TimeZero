{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/grpo-dataset-v1/antlr4_python3_runtime-4.13.2-py3-none-any.whl\n",
      "Installing collected packages: antlr4-python3-runtime\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.13.2\n",
      "    Uninstalling antlr4-python3-runtime-4.13.2:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.13.2\n",
      "Successfully installed antlr4-python3-runtime-4.13.2\n",
      "Processing /kaggle/input/grpo-dataset-v1/av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Installing collected packages: av\n",
      "  Attempting uninstall: av\n",
      "    Found existing installation: av 14.4.0\n",
      "    Uninstalling av-14.4.0:\n",
      "      Successfully uninstalled av-14.4.0\n",
      "Successfully installed av-14.4.0\n",
      "Processing /kaggle/input/grpo-dataset-v1/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e5f7a039fc0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e5f7a03a170>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e5f7a03a680>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Processing /kaggle/input/grpo-dataset-v1/blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/grpo-dataset-v1/antlr4_python3_runtime-4.13.2-py3-none-any.whl  \n",
    "!pip install /kaggle/input/grpo-dataset-v1/av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/compressed_tensors-0.9.3-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/hjson-3.1.0-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/einops-0.8.1-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/deepspeed-0.16.5-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/starlette-0.37.2-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/fastapi-0.115.12-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/transformers-4.51.3-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/vllm-0.8.5.post1-cp38-abi3-manylinux1_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/filelock-3.18.0-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/flash_attn-2.7.42Bcu124torch2.6.0cxx11abiFALSE-cp312-cp312-win_amd64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/huggingface_hub-0.31.2-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/jinja2-3.1.6-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/latex2sympy2_extended-1.10.1-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/math_verify-0.7.0-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/opencv_python_headless-4.11.0.86-cp37-abi3-macosx_13_0_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/partial_json_parser-0.2.1.1.post5-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/peft-0.15.2-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/python_multipart-0.0.20-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/qwen_vl_utils-0.0.11-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/ray-2.46.0-cp310-cp310-manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/rich_toolkit-0.14.6-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/sympy-1.13.1-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/torchao-0.11.0-cp39-abi3-manylinux_2_28_x86_64.manylinux_2_24_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/trl-0.17.0-py3-none-any.whl \n",
    "!pip install /kaggle/input/grpo-dataset-v1/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "input_file = \"/kaggle/input/grpo-dataset/unique_videos.jsonl\"\n",
    "MODEL_NAME = \"/kaggle/input/qwen2.5-vl/transformers/3b-instruct/2\"\n",
    "\n",
    "\n",
    "def get_duration(video_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        # raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "        print(f\"Video file not found: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cap.release()\n",
    "\n",
    "    if fps > 0:\n",
    "        duration = frame_count / fps\n",
    "    else:\n",
    "        duration = 0\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def preprocess_video_inner(video_path, processor, max_pixels, min_pixels):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"video\",\n",
    "                \"video\": video_path,\n",
    "                \"total_pixels\": max_pixels,\n",
    "                \"min_pixels\": min_pixels,\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "    fps_inputs = video_kwargs['fps']\n",
    "    return image_inputs, video_inputs, video_kwargs, fps_inputs\n",
    "\n",
    "def process_single_video(task_args):\n",
    "    video_path, processor, max_pixels, min_pixels, example_output_dir, sentence, solution, duration = task_args\n",
    "    image_inputs, video_inputs, video_kwargs, fps_inputs = preprocess_video_inner(video_path, processor, max_pixels, min_pixels)\n",
    "\n",
    "\n",
    "    example_output_dir = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    example_output_dir = f\"./dataset/train/{example_output_dir}\"\n",
    "    os.makedirs(example_output_dir, exist_ok=True)\n",
    "    torch.save(video_inputs, os.path.join(example_output_dir, \"video_inputs.pt\"))\n",
    "    with open(os.path.join(example_output_dir, \"video_kwargs.json\"), 'w') as f:\n",
    "        json.dump(video_kwargs, f)\n",
    "\n",
    "    return {\n",
    "            \"problem\": sentence,\n",
    "            \"solution\": solution,\n",
    "            \"preprocessed_path\": example_output_dir,\n",
    "            \"duration\": duration,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def process_split(file_path, split_name, output_dir, max_pixels, min_pixels, processor):\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    output_split_dir = os.path.join(output_dir, split_name)\n",
    "    os.makedirs(output_split_dir, exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = [json.loads(line.strip()) for line in f]\n",
    "    data = data[: 10]\n",
    "    examples = []\n",
    "    tasks = []\n",
    "\n",
    "    for video in tqdm(data):\n",
    "        timestamps = video[\"timestamp\"]\n",
    "        sentence = video[\"caption\"].strip().lower()\n",
    "        if sentence.endswith(\".\"):\n",
    "            sentence = sentence[:-1]\n",
    "\n",
    "        video_path = video[\"video\"]\n",
    "        video_id = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        example_output_dir = os.path.join(output_split_dir, f\"{video_id}\")\n",
    "        duration = get_duration(video[\"video\"])\n",
    "        solution = (float(timestamps[0]) / duration, float(timestamps[1]) / duration)\n",
    "\n",
    "        tasks.append((video_path, processor, max_pixels, min_pixels, example_output_dir, sentence, solution, duration))\n",
    "    print(\"task has ended\")\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/grpo-dataset-v1/decord-0.6.0-py3-none-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from decord==0.6.0) (1.26.4)\n",
      "\u001b[33mWARNING: Error parsing requirements for numba: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/numba-0.60.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: decord\n",
      "Successfully installed decord-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/grpo-dataset-v1/decord-0.6.0-py3-none-manylinux2010_x86_64.whl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "# !pip install /kaggle/input/grpo-dataset-v1/transformers-4.51.3-py3-none-any.whl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/grpo-dataset-v1/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "\u001b[33mWARNING: Error parsing requirements for numba: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/numba-0.60.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.8.3 requires cubinlinker, which is not installed.\n",
      "cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 24.8.3 requires numba>=0.57, which is not installed.\n",
      "cudf 24.8.3 requires ptxcompiler, which is not installed.\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 24.8.0 requires numba>=0.57, which is not installed.\n",
      "dask-cuda 24.8.2 requires numba>=0.57, which is not installed.\n",
      "dask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "datashader 0.16.3 requires numba, which is not installed.\n",
      "librosa 0.10.2.post1 requires numba>=0.51.0, which is not installed.\n",
      "raft-dask 24.8.1 requires numba>=0.57, which is not installed.\n",
      "rmm 24.8.2 requires numba>=0.57, which is not installed.\n",
      "shap 0.44.1 requires numba, which is not installed.\n",
      "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
      "ucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\n",
      "vllm 0.8.5.post1 requires numba==0.61.2; python_version > \"3.9\", which is not installed.\n",
      "ydata-profiling 4.10.0 requires numba<1,>=0.56.0, which is not installed.\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "cesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "cudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\n",
      "datasets 3.0.1 requires fsspec[http]<=2024.6.1,>=2023.1.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "rmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install  /kaggle/input/grpo-dataset-v1/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"02ba155e26496a78f062f683274330566fefe94c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/TimeZero\n"
     ]
    }
   ],
   "source": [
    "%cd TimeZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[2025-05-17 10:22:42,711] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-17 10:22:42,711] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-17 10:22:42,711] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-17 10:22:42,711] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "df: df: /root/.triton/autotunedf: /root/.triton/autotunedf: /root/.triton/autotune: No such file or directory: No such file or directory/root/.triton/autotune: No such file or directory\n",
      "\n",
      ": No such file or directory\n",
      "\n",
      "INFO 05-17 10:22:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-17 10:22:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-17 10:22:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-17 10:22:47 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-17 10:22:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-17 10:22:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-17 10:22:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-17 10:22:47 [__init__.py:239] Automatically detected platform cuda.\n",
      "[2025-05-17 10:22:49,951] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-05-17 10:22:49,951] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-05-17 10:22:49,951] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-05-17 10:22:49,951] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-05-17 10:22:49,951] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 317, in <module>\n",
      "[rank0]:     main(script_args, training_args, model_args)\n",
      "[rank0]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 231, in main\n",
      "[rank0]:     dataset = load_json_dataset(\n",
      "[rank0]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 177, in load_json_dataset\n",
      "[rank0]:     processor = AutoProcessor.from_pretrained(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py\", line 347, in from_pretrained\n",
      "[rank0]:     return processor_class.from_pretrained(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1079, in from_pretrained\n",
      "[rank0]:     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1143, in _get_arguments_from_pretrained\n",
      "[rank0]:     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 579, in from_pretrained\n",
      "[rank0]:     raise ValueError(\n",
      "[rank0]: ValueError: Unrecognized image processor in /kaggle/input/qwen2.5-vl/transformers/3b-instruct/2. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, aria, beit, bit, blip, blip-2, bridgetower, chameleon, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, fuyu, gemma3, git, glpn, got_ocr2, grounding-dino, groupvit, hiera, idefics, idefics2, idefics3, ijepa, imagegpt, instructblip, instructblipvideo, kosmos-2, layoutlmv2, layoutlmv3, levit, llama4, llava, llava_next, llava_next_video, llava_onevision, mask2former, maskformer, mgp-str, mistral3, mllama, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, nougat, oneformer, owlv2, owlvit, paligemma, perceiver, phi4_multimodal, pix2struct, pixtral, poolformer, prompt_depth_anything, pvt, pvt_v2, qwen2_5_vl, qwen2_vl, regnet, resnet, rt_detr, sam, segformer, seggpt, shieldgemma2, siglip, siglip2, superglue, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, timm_wrapper, tvlt, tvp, udop, upernet, van, videomae, vilt, vipllava, vit, vit_hybrid, vit_mae, vit_msn, vitmatte, xclip, yolos, zoedepth\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 317, in <module>\n",
      "[rank1]:     main(script_args, training_args, model_args)\n",
      "[rank1]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 231, in main\n",
      "[rank1]:     dataset = load_json_dataset(\n",
      "[rank1]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 177, in load_json_dataset\n",
      "[rank1]:     processor = AutoProcessor.from_pretrained(\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py\", line 347, in from_pretrained\n",
      "[rank1]:     return processor_class.from_pretrained(\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1079, in from_pretrained\n",
      "[rank1]:     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1143, in _get_arguments_from_pretrained\n",
      "[rank1]:     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 579, in from_pretrained\n",
      "[rank1]:     raise ValueError(\n",
      "[rank1]: ValueError: Unrecognized image processor in /kaggle/input/qwen2.5-vl/transformers/3b-instruct/2. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, aria, beit, bit, blip, blip-2, bridgetower, chameleon, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, fuyu, gemma3, git, glpn, got_ocr2, grounding-dino, groupvit, hiera, idefics, idefics2, idefics3, ijepa, imagegpt, instructblip, instructblipvideo, kosmos-2, layoutlmv2, layoutlmv3, levit, llama4, llava, llava_next, llava_next_video, llava_onevision, mask2former, maskformer, mgp-str, mistral3, mllama, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, nougat, oneformer, owlv2, owlvit, paligemma, perceiver, phi4_multimodal, pix2struct, pixtral, poolformer, prompt_depth_anything, pvt, pvt_v2, qwen2_5_vl, qwen2_vl, regnet, resnet, rt_detr, sam, segformer, seggpt, shieldgemma2, siglip, siglip2, superglue, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, timm_wrapper, tvlt, tvp, udop, upernet, van, videomae, vilt, vipllava, vit, vit_hybrid, vit_mae, vit_msn, vitmatte, xclip, yolos, zoedepth\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 317, in <module>\n",
      "[rank3]:     main(script_args, training_args, model_args)\n",
      "[rank3]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 231, in main\n",
      "[rank3]:     dataset = load_json_dataset(\n",
      "[rank3]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 177, in load_json_dataset\n",
      "[rank3]:     processor = AutoProcessor.from_pretrained(\n",
      "[rank3]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py\", line 347, in from_pretrained\n",
      "[rank3]:     return processor_class.from_pretrained(\n",
      "[rank3]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1079, in from_pretrained\n",
      "[rank3]:     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "[rank3]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1143, in _get_arguments_from_pretrained\n",
      "[rank3]:     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n",
      "[rank3]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 579, in from_pretrained\n",
      "[rank3]:     raise ValueError(\n",
      "[rank3]: ValueError: Unrecognized image processor in /kaggle/input/qwen2.5-vl/transformers/3b-instruct/2. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, aria, beit, bit, blip, blip-2, bridgetower, chameleon, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, fuyu, gemma3, git, glpn, got_ocr2, grounding-dino, groupvit, hiera, idefics, idefics2, idefics3, ijepa, imagegpt, instructblip, instructblipvideo, kosmos-2, layoutlmv2, layoutlmv3, levit, llama4, llava, llava_next, llava_next_video, llava_onevision, mask2former, maskformer, mgp-str, mistral3, mllama, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, nougat, oneformer, owlv2, owlvit, paligemma, perceiver, phi4_multimodal, pix2struct, pixtral, poolformer, prompt_depth_anything, pvt, pvt_v2, qwen2_5_vl, qwen2_vl, regnet, resnet, rt_detr, sam, segformer, seggpt, shieldgemma2, siglip, siglip2, superglue, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, timm_wrapper, tvlt, tvp, udop, upernet, van, videomae, vilt, vipllava, vit, vit_hybrid, vit_mae, vit_msn, vitmatte, xclip, yolos, zoedepth\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 317, in <module>\n",
      "[rank2]:     main(script_args, training_args, model_args)\n",
      "[rank2]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 231, in main\n",
      "[rank2]:     dataset = load_json_dataset(\n",
      "[rank2]:   File \"/kaggle/working/TimeZero/src/open_r1/grpo_video.py\", line 177, in load_json_dataset\n",
      "[rank2]:     processor = AutoProcessor.from_pretrained(\n",
      "[rank2]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py\", line 347, in from_pretrained\n",
      "[rank2]:     return processor_class.from_pretrained(\n",
      "[rank2]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1079, in from_pretrained\n",
      "[rank2]:     args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "[rank2]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py\", line 1143, in _get_arguments_from_pretrained\n",
      "[rank2]:     args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n",
      "[rank2]:   File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 579, in from_pretrained\n",
      "[rank2]:     raise ValueError(\n",
      "[rank2]: ValueError: Unrecognized image processor in /kaggle/input/qwen2.5-vl/transformers/3b-instruct/2. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, aria, beit, bit, blip, blip-2, bridgetower, chameleon, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, fuyu, gemma3, git, glpn, got_ocr2, grounding-dino, groupvit, hiera, idefics, idefics2, idefics3, ijepa, imagegpt, instructblip, instructblipvideo, kosmos-2, layoutlmv2, layoutlmv3, levit, llama4, llava, llava_next, llava_next_video, llava_onevision, mask2former, maskformer, mgp-str, mistral3, mllama, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, nougat, oneformer, owlv2, owlvit, paligemma, perceiver, phi4_multimodal, pix2struct, pixtral, poolformer, prompt_depth_anything, pvt, pvt_v2, qwen2_5_vl, qwen2_vl, regnet, resnet, rt_detr, sam, segformer, seggpt, shieldgemma2, siglip, siglip2, superglue, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, timm_wrapper, tvlt, tvp, udop, upernet, van, videomae, vilt, vipllava, vit, vit_hybrid, vit_mae, vit_msn, vitmatte, xclip, yolos, zoedepth\n",
      "[rank0]:[W517 10:22:51.432706327 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0517 10:22:52.901000 1220 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1317 closing signal SIGTERM\n",
      "W0517 10:22:52.901000 1220 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1318 closing signal SIGTERM\n",
      "W0517 10:22:52.901000 1220 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1319 closing signal SIGTERM\n",
      "E0517 10:22:52.997000 1220 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 1316) of binary: /opt/conda/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1165, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 799, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/open_r1/grpo_video.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-17_10:22:52\n",
      "  host      : 4131460c6668\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1316)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!git pull\n",
    "!bash scripts/run_grpo_video.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c0cdc046c14cb89468511a5992ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0e0090561240779582cda71fbd43e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770921c2c7914833b7bdb51e80f8a256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdb14af084e4bcb803cf855bc951053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ede86b0ead441e998b8ea9469ebfc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f634a900c44bbcaffbcd7352fd52b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLProcessor:\n",
       "- image_processor: Qwen2VLImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"max_pixels\": 12845056,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_pixels\": 3136,\n",
       "  \"patch_size\": 14,\n",
       "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"longest_edge\": 12845056,\n",
       "    \"shortest_edge\": 3136\n",
       "  },\n",
       "  \"temporal_patch_size\": 2\n",
       "}\n",
       "\n",
       "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "AutoProcessor.from_pretrained(pretrained_model_name_or_path = \"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
